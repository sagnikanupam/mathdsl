from __future__ import annotations
import pandas as pd
import json
import dreamcoder.domains.math.mathPrimitives as mdp
import dreamcoder.domains.conpole.conpoleMathPrimitives as cdp
import dreamcoder.domains.lemma.lemmaMathPrimitives as ldp
import math_domain_utils as mdu

ORIGINAL_DATASET_FILEPATHS = ['meta_analysis/MathDomainAnalysis/conpoleDatasetPrefix.csv']
ORIGINAL_CONPOLE_SOLNS = 'socratic-tutor/outputs/generatedConpoleSolutions.csv'
ORIGINAL_LEMMA_SOLNS = 'socratic-tutor/outputs/generatedLemmaSolutions.csv'
GENERATED_CONPOLE_SOLNS = 'meta_analysis/MathDomainAnalysis/generatedConpoleSolutions-CScores.csv' #After compute metrics has been called on the solutions
GENERATED_LEMMA_SOLNS = 'meta_analysis/MathDomainAnalysis/generatedLemmaSolutions-CScores.csv' #After compute metrics has been called on the solutions
DREAMCODER_STITCH_MATHDSL_JSON = 'meta_analysis/MathDomainAnalysis/dreamcoder_stitch_mathdsl.json'
DREAMCODER_STITCH_CONPOLEDSL_JSON = 'meta_analysis/MathDomainAnalysis/dreamcoder_stitch_conpoledsl.json'
DREAMCODER_STITCH_LEMMADSL_JSON = 'meta_analysis/MathDomainAnalysis/dreamcoder_stitch_lemmadsl.json'
AUTO_AUGMENTATION_JSON = []
ORIGINAL_TRAIN_SPLIT = 0.7
RANDOM_SEED = 111

functions_dict = {
    '(_swap': (mdp._swap, 2), 
    '(_add': (mdp._add, 2), 
    '(_sub': (mdp._sub, 2), 
    '(_mult': (mdp._mult, 2), 
    '(_div': (mdp._div, 2), 
    '(_rrotate': (mdp._rrotate, 2), 
    '(_lrotate': (mdp._lrotate, 2), 
    '(_simplify': (mdp._simplify, 2), 
    '(_dist': (mdp._dist, 2),
    '(_revdist': (mdp._revdist, 2),
    '(_addzero': (mdp._addzero, 2),
    '(_subzero': (mdp._subzero, 2),
    '(_multone': (mdp._multone, 2),
    '(_divone': (mdp._divone, 2),
    '(_newConstGen': (mdp._newConstGen, 3),
    '(_crefl': (cdp._refl, 2),
    '(_ccomm': (cdp._comm, 2),
    '(_cassoc': (cdp._assoc, 2),
    '(_cdist': (cdp._dist, 2),
    '(_csubcomm': (cdp._subcomm, 2),
    '(_ceval': (cdp._eval, 2),
    '(_caddzero': (cdp._addzero, 2),
    '(_csubzero': (cdp._subzero, 2),
    '(_cmultone': (cdp._multone, 2),
    '(_cdivone': (cdp._divone, 2),
    '(_cdivself': (cdp._divself, 2),
    '(_csubself': (cdp._subself, 2),
    '(_csubsub': (cdp._subsub, 2),
    '(_cmultzero': (cdp._multzero, 2),
    '(_czerodiv': (cdp._zerodiv, 2),
    '(_cadd': (cdp._add, 2),
    '(_csub': (cdp._sub, 2),
    '(_cmult': (cdp._mult, 2),
    '(_cdiv': (cdp._div, 2),
    '(_cnewConstGen': (cdp._newConstGen, 3),
    '(_assoc_eval_add0': (ldp._assoc_eval_add0, 2),
    '(_assoc_eval_eval_add0': (ldp._assoc_eval_eval_add0, 2),
    '(_sub_eval_comm': (ldp._sub_eval_comm, 2),
    '(_dist_dist_eval_eval_eval_eval_multone': (ldp._dist_dist_eval_eval_eval_eval_multone, 2),
    '(_add_eval_comm_assoc_comm': (ldp._add_eval_comm_assoc_comm, 2),
    '(_sub_eval_comm_assoc_eval_add0': (ldp._sub_eval_comm_assoc_eval_add0, 2),
    '(_add_eval_comm_assoc_comm_assoc_eval_add0': (ldp._add_eval_comm_assoc_comm_assoc_eval_add0, 2),
    '(_sub_assoc_eval_eval_add0_multone': (ldp._sub_assoc_eval_eval_add0_multone, 2),
    '(_sub_assoc_eval_eval_add0': (ldp._sub_assoc_eval_eval_add0, 2),
    '(_sub_subsub_comm_assoc_eval_eval_add0_multone': (ldp._sub_subsub_comm_assoc_eval_eval_add0_multone, 2),
    '(_div_eval_comm_assoc_eval_multone': (ldp._div_eval_comm_assoc_eval_multone, 2),
    '(_div_comm_assoc_eval': (ldp._div_comm_assoc_eval, 2),
    '(_div_comm_assoc_eval_eval_multone': (ldp._div_comm_assoc_eval_eval_multone, 2),
    '(_sub_dist_comm_assoc_subself_eval': (ldp._sub_dist_comm_assoc_subself_eval, 2),
    '(_eval_eval': (ldp._eval_eval, 2)
}

def replacements(s: str):
    """
    Given a string of a program generated by DreamCoder, replaces all mathDomain primitive names with their corresponding functions_dict index name.

    Arguments: 
        s(str): contains a lambda expression describing a program generated by DreamCoder which is a series of functions composed of primitives. E.g. - (lambda (#(lambda (mathDomain_simplify (mathDomain_dist $0 1) 0)) $0)) 
    
    Returns:
        a string with all the mathDomain primitive names replaced. This output shall henceforth be referred to as a solution program expression.

    """
    replacements_dict = {'mathDomain_swap': '_swap', 'mathDomain_add': '_add', 'mathDomain_sub': '_sub', 'mathDomain_mult': '_mult', 'mathDomain_div': '_div', 'mathDomain_rrotate': '_rrotate', 'mathDomain_lrotate': '_lrotate', 'mathDomain_simplify': '_simplify', 'mathDomain_dist':'_dist', 'mathDomain_revdist': '_revdist', 'mathDomain_addzero': '_addzero', 'mathDomain_subzero': '_subzero', 'mathDomain_multone': '_multone', 'mathDomain_divone': '_divone', 'mathDomain_newConstGen': '_newConstGen',
    'conpole_refl': '_crefl', 'conpole_comm': '_ccomm', 'conpole_assoc': '_cassoc', 'conpole_dist': '_cdist', 'conpole_subcomm': '_csubcomm', 'conpole_eval': '_ceval', 'conpole_addzero': '_caddzero', 'conpole_subzero': '_csubzero', 'conpole_multone': '_cmultone', 'conpole_divone': '_cdivone', 'conpole_divself': '_cdivself', 'conpole_subself': '_csubself', 'conpole_subsub': '_csubsub', 'conpole_multzero': '_cmultzero', 'conpole_zerodiv': '_czerodiv', 'conpole_add': '_cadd', 'conpole_sub': '_csub', 'conpole_mult': '_cmult', 'conpole_div': '_cdiv', 'conpole_newConstGen': '_cnewConstGen', 
    'lemma_assoc_eval_add0': '_assoc_eval_add0', 'lemma_assoc_eval_eval_add0': '_assoc_eval_eval_add0', 'lemma_sub_eval_comm': '_sub_eval_comm', 'lemma_dist_dist_eval_eval_eval_eval_multone': '_dist_dist_eval_eval_eval_eval_multone', 'lemma_add_eval_comm_assoc_comm': '_add_eval_comm_assoc_comm', 'lemma_sub_eval_comm_assoc_eval_add0': '_sub_eval_comm_assoc_eval_add0', 'lemma_add_eval_comm_assoc_comm_assoc_eval_add0': '_add_eval_comm_assoc_comm_assoc_eval_add0', 'lemma_sub_assoc_eval_eval_add0_multone': '_sub_assoc_eval_eval_add0_multone', 'lemma_sub_assoc_eval_eval_add0': '_sub_assoc_eval_eval_add0', 'lemma_sub_subsub_comm_assoc_eval_eval_add0_multone': '_sub_subsub_comm_assoc_eval_eval_add0_multone', 'lemma_div_eval_comm_assoc_eval_multone': '_div_eval_comm_assoc_eval_multone', 'lemma_div_comm_assoc_eval': '_div_comm_assoc_eval', 'lemma_div_comm_assoc_eval_eval_multone': '_div_comm_assoc_eval_eval_multone', 'lemma_sub_dist_comm_assoc_subself_eval': '_sub_dist_comm_assoc_subself_eval', 'lemma_eval_eval': '_eval_eval',
    }
    for i in range(10):
        replacements_dict['mathDomain_'+str(i)] = str(i)
    for or_text in replacements_dict.keys():
        s = s.replace(or_text, replacements_dict[or_text])
    return s

def clSolnEval(clSolnPath, outputSolnPath, no_duplicates):
    '''
    Given a .csv file containing solutions generated by ConPoLe or Lemma, convert the solutions into lists of steps in prefix notation, and then compute each solution's metric function.
    
    Arguments:
        clSolnPath (str): A string containing a path to the .csv file containing the ConPoLe/Lemma solutions.
        outputSolnPath (str): A string containing a path to the .csv where we would like to store our ConPoLe/Lemma solutions alongside their metric function values.
        no_duplicates (bool): A boolean denoting whether to remove duplicate steps in the solution or not. If True, the solution steps will be unique, except for one-step solutions (i.e. if the input equation is x=1, steps will be (x=1 => x=1)). If False, the solution steps will be as they are in the input file.

    Returns:
        None
    '''
    df = pd.read_csv(clSolnPath)
    metrics = []
    for i in range(df.shape[0]):
        str_soln = df.loc[i]['soln']
        str_soln = str_soln.replace("[", "(")
        str_soln = str_soln.replace("]", ")")
        str_eq = df.loc[i]['eqn']
        infix_steps = str_soln.split('|')
        #steps = [str_eq]+steps
        steps = []
        if no_duplicates:
            for step in infix_steps:
                if step not in steps:
                    steps.append(step)
            if len(steps)<2:
                steps = [str_eq]+steps
        else:
            steps = infix_steps
        prefix_steps = [mdu.infix_to_prefix_conversion(step) for step in steps]
        metrics.append(computeMetrics(prefix_steps))
    df['metrics'] = metrics
    df.to_csv(outputSolnPath, index=False)

def matchBr(s: str, ind: int) -> int | None:
    """
    Given an opening bracket at position ind in string s, find the  position of the corresponding closing bracket.

    Arguments: 
    s (str): denoting the solution program expression (already processed by replacements())
    ind (int): is an integer denoting the starting position of the start bracket '('

    Returns: 
    int | None: an integer denoting position of closing bracket. If start index does not have an open bracket or no closing brackets close the starting bracket, returns None.
    """
    brPair = 0
    for j in range(ind, len(s)):
        if s[j]=="(":
            brPair+=1
        if s[j]==")":
            brPair-=1
        if brPair==0:
            if j==ind:
                return None
            return j
    return None

def evaluate(s: str, args: list[str], depth: int):
    """
    Given a solution program expression, generate mathematical solutions as a list of prefix expressions, stored as a string separated by |.

    Arguments: 
        s (str): A string denoting the solution program expression (already processed by replacements())
        args (list[str]): A string containing an equation as a prefix-tree expression. This is the equation to be solved.
        depth (int): A parameter to modulate granularity of solutions, only functions with #lambdas<depth have the results of their argument included in the solution
    
    Returns:
        tuple: a tuple (list of evaluated function arguments, prefix expression generated by evaluating the function).
    """
    try:
        init_split = s.split(" ")
        
        #This is a hacky way to ensure (via type-checking) that the order of the arguments is correct. Any function in the DSL that requires two arguments will have the second argument be an integer index to the subtree the operation is acting on. This is used because programs that mention "$1" in functional argument syntax are using inner-loop and outer-loop arguments.
        if (len(args)>1 and mdp.intConvertable(args[0])):
            args = [args[1], args[0]]
            
        if s=="":
            return args
        if args == [] or args[0]=="":
            return ([s], [s])
        if init_split[0]=="$1":
            return ([str(args[1])], [str(args[1])])
        if init_split[0] == "$0":
            return ([str(args[0])], [str(args[0])])
        if init_split[0].isnumeric():
            return ([str(int(init_split[0]))], [str(int(init_split[0]))])
        if init_split[0]=="(lambda" or init_split[0]=="(#(lambda":
            subExpStart = 8 if init_split[0]=="(lambda" else 2
            funcEnd = matchBr(s, subExpStart)
            if funcEnd!=None:
                funcArgs = []
                newFunc = s[subExpStart:funcEnd+1]
                currArgStart = funcEnd+2
                while (currArgStart < len(s)-1):
                    if s[currArgStart]=="(":
                        argEnd = matchBr(s, currArgStart)
                    else:
                        argEnd = None
                    if argEnd==None:
                        if currArgStart!=len(s)-1:
                            newArg =  s[currArgStart:-1].split(" ")[0]
                            funcArgs.append(newArg)
                            currArgStart += len(newArg)+1 
                            continue
                        else:
                            break
                    else:
                        funcArgs.append(s[currArgStart:argEnd+1])
                        currArgStart = argEnd+2
                #print(f"Arguments to the function {s[subExpStart:funcEnd+1]} are: {funcArgs} \n")
                currFuncArgs = []
                for funcArg in funcArgs:
                    currFuncArgs += evaluate(funcArg, args, depth+1)[0]
                #print(f"Evaluated arguments to the function {s[subExpStart:funcEnd+1]} are: {currFuncArgs} \n")
                if currFuncArgs == []:
                    return evaluate(newFunc, args, depth)
                else:
                    result_tuple = evaluate(newFunc, currFuncArgs, depth+1)
                    result_tuple[1].append(currFuncArgs[0])
                    return result_tuple
            else:
                newFunc = s[subExpStart:-1]
                #print(f"New Func is: {newFunc} \n")
                return evaluate(newFunc, args, depth+1)
        else:
            if init_split[0] in functions_dict.keys():
                currArgStart = len(init_split[0])+1
                funcArgs = []
                while (currArgStart < len(s)-1):
                    if s[currArgStart]=="(":
                        argEnd = matchBr(s, currArgStart)
                    else:
                        argEnd = None
                    if argEnd==None:
                        if currArgStart!=len(s)-1:
                            newArg =  s[currArgStart:-1].split(" ")[0]
                            funcArgs.append(newArg)
                            currArgStart += len(newArg)+1 
                            continue
                        else:
                            break
                    else:
                        funcArgs.append(s[currArgStart:argEnd+1])
                        currArgStart = argEnd+2
                #print(f"Arguments to the function {init_split[0]} are: {funcArgs} \n")
                currFuncArgs = []
                for funcArg in funcArgs:
                    currFuncArgs += evaluate(funcArg, args, depth+1)[0]
                if funcArgs == ['$0', '$1']:
                    currFuncArgs = [currFuncArgs[1], currFuncArgs[0]]
                #print(f"Evaluated arguments to the function {init_split[0]} are: {currFuncArgs} \n")
                if len(currFuncArgs)==functions_dict[init_split[0]][1]:
                    if functions_dict[init_split[0]][1]==2:
                        try:
                            return ([functions_dict[init_split[0]][0](currFuncArgs[0], int(currFuncArgs[1]))], [functions_dict[init_split[0]][0](currFuncArgs[0], int(currFuncArgs[1]))])
                        except Exception as e:
                            print(f"Error encountered in program {s} while evaluating {init_split[0]} with args {[currFuncArgs[0], int(currFuncArgs[1])]}. \n")
                            print(e)
                            return ([args[0]], [args[0]])
                    elif functions_dict[init_split[0]][1]==3:
                        try:
                            return ([functions_dict[init_split[0]][0](int(currFuncArgs[0]), int(currFuncArgs[1]), int(currFuncArgs[2]))], [functions_dict[init_split[0]][0](int(currFuncArgs[0]), int(currFuncArgs[1]), int(currFuncArgs[2]))])
                        except Exception as e:
                            print(f"Error encountered in program {s} while evaluating {init_split[0]} with args {[currFuncArgs[0], int(currFuncArgs[1]), int(currFuncArgs[2])]}. \n")
                            print(e)
                            return ([args[0]], [args[0]])
                else:
                    #print(f"Miscounted arguments for {init_split[0]} with args {args} \n")
                    raise Exception("Miscounted arguments")
            else:
                return ([s], [s])
    except Exception as e:
        print(e)
        print(f"Error encountered in program {s} with args {args} \n")
        return args[0]

def computeMetrics(steps):
    """
    Takes a list of solution steps (in prefix format) as input and computes their conciseness metric function value, f(s)

    Arguments:
        steps (list[str]): A list of strings containing equations in prefix format, which can be accepted by mdp.treefy(). The steps must be in order i.e. steps[0] must be the first step in the solution, steps[1] must be the second step in the solution and so on.

    Returns:
        int: an integer denoting the total value of the metric function for that equation
    """
    total_metric = 0
    for ind in range(1, len(steps)):
        total_metric+=mdp._metric(steps[ind-1],steps[ind])
    return total_metric

def computeDreamCoderCScores(test_problem_df: pd.DataFrame, model_result_json: str, baseline_model_result_df: pd.DataFrame, experiment_name: str, no_duplicates: bool):
    """
    Compute the average conciseness scores of a DreamCoder experiment on the problems solved by the baseline model (ConPoLe/Lemma) in baseline_model_result_df. Ignore any problems solved by DreamCoder that are not solved by the baseline model. 

    Args:
        test_problem_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_json (str): JSON filename (frontiers.json in the last iteration of the experiment) containing results of DreamCoder.
        baseline_model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma.
        experiment_name (str): experiment name to be printed.
        no_duplicates (bool): A boolean denoting whether to remove duplicate steps in the solution or not. If True, the solution steps will be unique, except for one-step solutions (i.e. if the input equation is x=1, steps will be (x=1 => x=1)). If False, the solution steps will be as they are in the input file.

    Returns:
        float: Average conciseness score of the DreamCoder experiment on the problems solved by the baseline model.
        
    """
    total_metric = 0
    mutual_problems = 0
    test_problem_df["tmp"] = test_problem_df.index
    test_problem_df["dreamcoder_steps"] = ["unknown"]*len(test_problem_df)
    test_problem_df["dreamcoder_metrics"] = ["unknown"]*len(test_problem_df)
    test_problem_df["dreamcoder_cscores"] = ["unknown"]*len(test_problem_df)
    model_result_equation_nums = baseline_model_result_df["Equation Number"].to_list()
    test_problem_df.reset_index(drop=True, inplace=True)
    dreamcoder_solved_problems = []
    with open(model_result_json, "r") as f:
        model_result = json.load(f)
        for problem in model_result["train"].keys():
            equation = model_result["train"][problem]["task"].split("=>")[0]
            if len(model_result["train"][problem]["programs"]) > 0:
                dreamcoder_solved_problems.append((equation[len(equation.split(" ")[0])+1:], model_result["train"][problem]["programs"][0]["program"]))
        for problem in model_result["test"].keys():
            equation = model_result["test"][problem]["task"].split("=>")[0]
            if len(model_result["test"][problem]["programs"]) > 0:
                dreamcoder_solved_problems.append((equation[len(equation.split(" ")[0])+1:], model_result["test"][problem]["programs"][0]["program"]))
    for index in range(len(test_problem_df)):
        problem_name = test_problem_df["Prefix_Eq"].iloc[index]
        f_baseline = None                                                 
        if test_problem_df["tmp"].iloc[index] in model_result_equation_nums :
            for baseline_index in range(len(baseline_model_result_df)):
                if baseline_model_result_df["Equation Number"].iloc[baseline_index] == test_problem_df["tmp"].iloc[index]:
                    f_baseline = int(baseline_model_result_df["metrics"].iloc[baseline_index])
        if f_baseline != None:
            for solved_problem, program in dreamcoder_solved_problems:
                if solved_problem == problem_name:
                    try:
                        mutual_problems+=1
                        dreamcoder_solution = replacements(program)
                        prefix_steps = evaluate(dreamcoder_solution, [problem_name], 0)[1]
                        prefix_steps.reverse()
                        if no_duplicates:
                            #Ensure that the only abstraction outputs considered are equation states and not indices/constants
                            prefix_steps_base = [problem_name]+[step for step in prefix_steps if len(step)>1]
                            #Ensure that the steps are unique
                            prefix_steps = []
                            for step in prefix_steps_base:
                                if step not in prefix_steps:
                                    prefix_steps.append(step)
                            if len(prefix_steps)<2:
                                prefix_steps = [problem_name]+prefix_steps
                        else:
                            prefix_steps = [problem_name]+[step for step in prefix_steps if len(step)>1] 
                        test_problem_df.loc[index,"dreamcoder_steps"] = str('|'.join(prefix_steps))
                        f_model = computeMetrics(prefix_steps)
                        test_problem_df.loc[index,"dreamcoder_metrics"] = str(f_model)
                        total_metric+= (f_baseline-f_model)/f_baseline
                        test_problem_df.loc[index,"dreamcoder_cscores"] = str((f_baseline-f_model)/f_baseline)
                        break
                    except Exception as e:
                        print(e)
                        print(f"Error encountered while processing problem {problem_name}")
                        print(program)
                        continue
    print(f"For {experiment_name} and a total of {mutual_problems} number of mutual problems, the average conciseness score is: {total_metric/mutual_problems}")
    return total_metric/mutual_problems

def computeLemmaCScores(test_problem_df: pd.DataFrame, lemma_result_df: pd.DataFrame, baseline_model_result_df: pd.DataFrame, experiment_name: str):
    """
    Compute the average conciseness scores of a Lemma experiment on the problems solved by the baseline model (ConPoLe/Lemma) in baseline_model_result_df. Ignore any problems solved by Lemma that are not solved by the baseline model. 

    Args:
        test_problem_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        lemma_result_df (pd.DataFrame): Dataframe containing Lemma's problem solutions.
        baseline_model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma.
        experiment_name (str): experiment name to be printed.

    Returns:
        float: Average conciseness score of the DreamCoder experiment on the problems solved by the baseline model.
        
    """
    total_metric = 0
    mutual_problems = 0
    test_problem_df["tmp"] = test_problem_df.index
    test_problem_df["lemma_metrics"] = ["unknown"]*len(test_problem_df)
    test_problem_df["lemma_cscores"] = ["unknown"]*len(test_problem_df)
    baseline_result_equation_nums = baseline_model_result_df["Equation Number"].to_list()
    lemma_result_equation_nums = lemma_result_df["Equation Number"].to_list()
    test_problem_df.reset_index(drop=True, inplace=True)
    for index in range(len(test_problem_df)):
        equation_num = test_problem_df["tmp"].iloc[index]
        if equation_num in lemma_result_equation_nums and equation_num in baseline_result_equation_nums:
            mutual_problems+=1
            f_model = lemma_result_df[lemma_result_df["Equation Number"]==equation_num]["metrics"].iloc[0]
            f_baseline = baseline_model_result_df[baseline_model_result_df["Equation Number"]==equation_num]["metrics"].iloc[0]
            total_metric+= (f_baseline-f_model)/f_baseline
            test_problem_df.loc[index,"dreamcoder_cscores"] = str((f_baseline-f_model)/f_baseline)
    print(f"For {experiment_name} and a total of {mutual_problems} number of mutual problems, the average conciseness score is: {total_metric/mutual_problems}")
    return total_metric/mutual_problems

def checkLemmaConpoleAccuracy(test_problem_df:pd.DataFrame, model_result_df: pd.DataFrame, model_name: str):
    """
    Check accuracy of ConPoLe/Lemma model results

    Args:
        test_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma
        model_name (str): model name to be printed
    
    """
    count = 0
    test_problem_df["tmp"] = test_problem_df.index
    model_result_equation_nums = model_result_df["Equation Number"].to_list()
    for index in range(len(test_problem_df)):
        if test_problem_df["tmp"].iloc[index] in model_result_equation_nums:
            count+=1
    print(f"{model_name} Num Problems Solved: {count} / {len(test_problem_df)}")
    print(f"{model_name} Accuracy: {count / len(test_problem_df)}")

def checkDreamCoderAccuracy(test_problem_df:pd.DataFrame, model_result_json: str, experiment_name: str):
    """
    Check accuracy of a DreamCoder Experiment's results

    Args:
        test_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_json (str): JSON filename (frontiers.json in the last iteration of the experiment) containing results of DreamCoder.
        experiment_name (str): experiment name to be printed
    """
    count = 0
    solved_problems = []
    with open(model_result_json, "r") as f:
        model_result = json.load(f)
        for problem in model_result["train"].keys():
            if len(model_result["train"][problem]["programs"]) > 0:
                solved_problems.append(problem)
        for problem in model_result["test"].keys():
            if len(model_result["test"][problem]["programs"]) > 0:
                solved_problems.append(problem)
    for index in range(len(test_problem_df)):
        problem_name = test_problem_df["Prefix_Eq"].iloc[index]
        for solved_problem in solved_problems:
            if solved_problem.find(problem_name) != -1:
                count+=1
                break
    print(f"{experiment_name} Num Problems Solved: {count} / {len(test_problem_df)}")
    print(f"{experiment_name} Accuracy: {count / len(test_problem_df)}")

if __name__ =="__main__":
    NO_DUPLICATES = False
    clSolnEval(ORIGINAL_CONPOLE_SOLNS, GENERATED_CONPOLE_SOLNS, NO_DUPLICATES)
    clSolnEval(ORIGINAL_LEMMA_SOLNS, GENERATED_LEMMA_SOLNS, NO_DUPLICATES)
    allEqDatasets = [pd.read_csv(dataset) for dataset in ORIGINAL_DATASET_FILEPATHS]
    allEqDf = pd.concat([data for data in allEqDatasets], ignore_index=True)
    conpoleDf = pd.read_csv(GENERATED_CONPOLE_SOLNS)
    lemmaDf = pd.read_csv(GENERATED_LEMMA_SOLNS)
    numEqs = allEqDf.shape[0]
    numTrainSamples = int(numEqs*ORIGINAL_TRAIN_SPLIT)
    print("Number of training samples: ", numTrainSamples)
    print("Number of testing samples: ", numEqs - numTrainSamples)
    trainDf = allEqDf.sample(n=numTrainSamples, random_state=RANDOM_SEED)
    testDf = allEqDf.drop(trainDf.index).sample(frac=1, random_state=RANDOM_SEED)
    EXP_NAME = "DREAMCODER_STITCH_LEMMADSL" #Can be one of DREAMCODER_STITCH_MATHDSL, DREAMCODER_STITCH_CONPOLEDSL, DREAMCODER_STITCH_LEMMADSL, LEMMA
    if EXP_NAME == "DREAMCODER_STITCH_MATHDSL":
        computeDreamCoderCScores(trainDf, DREAMCODER_STITCH_MATHDSL_JSON, conpoleDf, "DreamCoder + Stitch + MathDSL (train)", NO_DUPLICATES)
        computeDreamCoderCScores(testDf, DREAMCODER_STITCH_MATHDSL_JSON, conpoleDf, "DreamCoder + Stitch + MathDSL (test)", NO_DUPLICATES)
        # trainDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderMathDSLTrainSolns.xlsx")
        # testDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderMathDSLTestSolns.xlsx")
    elif EXP_NAME == "DREAMCODER_STITCH_CONPOLEDSL":
        computeDreamCoderCScores(trainDf, DREAMCODER_STITCH_CONPOLEDSL_JSON, conpoleDf, "DreamCoder + Stitch + ConpoleDSL (train)", NO_DUPLICATES)
        computeDreamCoderCScores(testDf, DREAMCODER_STITCH_CONPOLEDSL_JSON, conpoleDf, "DreamCoder + Stitch + ConpoleDSL (test)", NO_DUPLICATES)
        # trainDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderConpoleDSLTrainSolns.xlsx")
        # testDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderConpoleDSLTestSolns.xlsx")
    elif EXP_NAME == "DREAMCODER_STITCH_LEMMADSL":
        computeDreamCoderCScores(trainDf, DREAMCODER_STITCH_LEMMADSL_JSON, conpoleDf, "DreamCoder + Stitch + LemmaDSL (train)", NO_DUPLICATES)
        computeDreamCoderCScores(testDf, DREAMCODER_STITCH_LEMMADSL_JSON, conpoleDf, "DreamCoder + Stitch + LemmaDSL (test)", NO_DUPLICATES)
        # trainDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderLemmaDSLTrainSolns.xlsx")
        # testDf.to_excel("meta_analysis/MathDomainAnalysis/DreamCoderLemmaDSLTestSolns.xlsx")
    elif EXP_NAME == "LEMMA":
        checkLemmaConpoleAccuracy(trainDf, conpoleDf, "ConPoLe")
        checkLemmaConpoleAccuracy(trainDf, lemmaDf, "Lemma")
        checkLemmaConpoleAccuracy(testDf, conpoleDf, "ConPoLe")
        checkLemmaConpoleAccuracy(testDf, lemmaDf, "Lemma")
        computeLemmaCScores(trainDf, lemmaDf, conpoleDf, "Lemma (train)")
        computeLemmaCScores(testDf, lemmaDf, conpoleDf, "Lemma (test)")

        # trainDf.to_excel("meta_analysis/MathDomainAnalysis/TrainLemmaCScores.xlsx")
        # testDf.to_excel("meta_analysis/MathDomainAnalysis/TestLemmaCScores.xlsx") 

    #Tests:    
    # s = replacements("(lambda (#(lambda (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_simplify (mathDomain_dist $0 mathDomain_2) mathDomain_3))) (#(lambda (lambda (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_mult (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_multone (mathDomain_swap $0 $1) mathDomain_2)) mathDomain_3) mathDomain_1) mathDomain_0) mathDomain_0) mathDomain_1) mathDomain_0) mathDomain_1))) mathDomain_0 $0)))")
    # steps: list[str] = evaluate(s, ["(= (+ (1) (* (2) (x))) (+ (-3) (* (-4) (x))))"], 0)[1]
    # steps.reverse()
    # s = "(lambda (#(lambda (#(lambda (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_simplify (mathDomain_dist $0 mathDomain_2) mathDomain_3))) (mathDomain_multone (mathDomain_lrotate (#(lambda (lambda (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_mult (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_multone (mathDomain_swap $0 $1) mathDomain_2)) mathDomain_3) mathDomain_1) mathDomain_0) mathDomain_0) mathDomain_1) mathDomain_0) mathDomain_1))) mathDomain_1 $0) mathDomain_1) mathDomain_6))) (mathDomain_swap $0 mathDomain_6)))"
    # eq = "(= (- (* (1) (x)) (2)) (- (* (3) (x)) (4))"
    #print(len(mdp._genSub(eq)))
    #print(computeMetrics(steps))
    # print(evaluate(replacements(s), [eq], 0))
    # s = replacements("(lambda (#(lambda (lambda (mathDomain_simplify (mathDomain_dist (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_dist (mathDomain_rrotate (mathDomain_swap (mathDomain_simplify (mathDomain_dist (mathDomain_rrotate (mathDomain_sub (mathDomain_rrotate (mathDomain_swap $0 $1) mathDomain_4) mathDomain_5) mathDomain_1) mathDomain_1) mathDomain_0) mathDomain_5) mathDomain_4) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_1) mathDomain_0))) mathDomain_5 (#(lambda (#(lambda (mathDomain_simplify (mathDomain_dist (mathDomain_swap $0 mathDomain_0) mathDomain_1) mathDomain_0)) (mathDomain_dist (mathDomain_lrotate (mathDomain_swap $0 mathDomain_1) mathDomain_1) mathDomain_2))) $0)))")
    # steps = evaluate(s, ["(= (- (+ (* (-1) (x)) (2)) (3)) (* (-4) (x)))"], 0)
    # print(steps)
    
    # s = replacements("(lambda (#(lambda (lambda (#(lambda (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate $0 mathDomain_4) mathDomain_0) mathDomain_0) mathDomain_3) mathDomain_5) mathDomain_4) mathDomain_0) mathDomain_0)) (mathDomain_swap (#(lambda (mathDomain_simplify (mathDomain_dist (mathDomain_rrotate $0 mathDomain_1) mathDomain_1) mathDomain_0)) (mathDomain_sub (mathDomain_multone (mathDomain_rrotate (#(lambda (mathDomain_simplify (mathDomain_dist (mathDomain_rrotate $0 mathDomain_1) mathDomain_1) mathDomain_0)) (mathDomain_sub (mathDomain_swap (#(lambda (mathDomain_swap (mathDomain_simplify $0 mathDomain_0) mathDomain_0)) $0) $1) mathDomain_5)) mathDomain_4) mathDomain_5) mathDomain_5)) mathDomain_5)))) mathDomain_1 $0))")
    # steps = evaluate(s, ["(= (+ (* (1) (x)) (2)) (+ (3) (* (-4) (x))))"], 0)
    # print(steps)
    
    # s = replacements("(lambda (#(lambda (#(lambda (lemma_div_eval_comm_assoc_eval_multone (lemma_assoc_eval_add0 $0 mathDomain_0) mathDomain_2)) (#(lambda (conpole_refl (lemma_eval_eval $0 mathDomain_2) mathDomain_0)) (conpole_dist $0 mathDomain_2)))) $0))")
    # steps = evaluate(s, ["(= (7) (- (* (6) (x)) (* (2) (x))))"], 0)
    # print(steps)